[core]
# The folder where airflow should store its log files
# base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

# The executor class that airflow should use.
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database.
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow

# The amount of parallelism as a setting to the executor
parallelism = 4

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 4

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 1

# Whether to load the examples that ship with Airflow
load_examples = False

[webserver]
# The base url of your website
base_url = http://localhost:8082

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Secret key to save connection in the db
secret_key = your_secret_key_here

[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5